{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ0i1mh8Gj5g",
        "outputId": "2300f34f-b6d0-4466-bd5a-a983081757b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted label for 'Football requires labour': 2\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample corpus and labels (replace with your actual data)\n",
        "corpus = [\n",
        "    \"The latest smartphone features a state-of-the-art camera.\",\n",
        "    \"Innovations in AI are transforming the tech industry.\",\n",
        "    \"The new tablet is lightweight and has a long battery life.\",\n",
        "    \"Tech companies are investing heavily in quantum computing.\",\n",
        "    \"Autonomous vehicles could revolutionize daily commutes.\",\n",
        "    \"The election campaign is heating up with debates on policy.\",\n",
        "    \"Legislation for environmental protection was recently passed.\",\n",
        "    \"The new tax law has sparked controversy among businesses.\",\n",
        "    \"Diplomatic talks will address the rising trade tensions.\",\n",
        "    \"The government pledged to increase funding for education.\",\n",
        "    \"The football team clinched victory with a last-minute goal.\",\n",
        "    \"A record-breaking performance in the 100 meters sprint.\",\n",
        "    \"The basketball playoffs are attracting large audiences.\",\n",
        "    \"A new swimming champion has emerged at the international meet.\",\n",
        "    \"The baseball game was postponed due to bad weather.\"\n",
        "]\n",
        "\n",
        "# 0 for Tech, 1 for Politics and 2 for Sport\n",
        "\n",
        "labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2)\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "\n",
        "# Transform training and testing data into TF-IDF vectors\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the SVM classifier\n",
        "clf = SVC(kernel='linear')  # Choose kernel (linear here for simplicity)\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on unseen data (replace with your new sentence)\n",
        "new_sentence = \"Football requires labour\"\n",
        "new_sentence_tfidf = vectorizer.transform([new_sentence])\n",
        "\n",
        "# Get prediction for unseen data\n",
        "prediction = clf.predict(new_sentence_tfidf)\n",
        "\n",
        "# Print the predicted label\n",
        "print(f\"Predicted label for '{new_sentence}': {prediction[0]}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5A9S9obGF-n",
        "outputId": "e621427e-38c7-44d3-9123-0fe1f6e8902d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The predicted label for the unseen sentence is: Positive\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Dataset with 20 texts and corresponding labels (1 for positive, 0 for negative)\n",
        "texts = [\n",
        "    \"I love this phone, its super fast and there's so much new stuff to learn!\",\n",
        "    \"The camera quality is amazing, I've taken some great shots.\",\n",
        "    \"I hate this phone, it's always crashing.\",\n",
        "    \"Battery life is terrible, it barely lasts a day.\",\n",
        "    \"Great phone, but I miss having a headphone jack.\",\n",
        "    \"The new update is awesome, my phone works much better now.\",\n",
        "    \"The screen broke after a minor fall, very fragile.\",\n",
        "    \"The voice assistant is surprisingly accurate and helpful!\",\n",
        "    \"I don't like the new interface, it's too complicated.\",\n",
        "    \"Fingerprint sensor is not responsive, I prefer the old scanner.\",\n",
        "    \"This laptop has excellent performance for gaming.\",\n",
        "    \"The sound quality on these headphones is not great.\",\n",
        "    \"I love how easy it is to take photos with this camera.\",\n",
        "    \"The battery on this laptop doesn't last long enough.\",\n",
        "    \"This gaming console is the best I've used so far.\",\n",
        "    \"The touchscreen is unresponsive at times.\",\n",
        "    \"This new app has really helped me organize my schedule.\",\n",
        "    \"The wifi connectivity on this device is poor.\",\n",
        "    \"I'm really impressed with the build quality of this tablet.\",\n",
        "    \"The picture quality on this TV is breathtaking.\"\n",
        "]\n",
        "labels = [1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a pipeline that combines TF-IDF with an SVM classifier\n",
        "pipeline = make_pipeline(TfidfVectorizer(), SVC(kernel='linear'))\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the topic of an unseen sentence\n",
        "unseen_sentence = \"This smartwatch has excellent battery life\"\n",
        "predicted_label = pipeline.predict([unseen_sentence])[0]\n",
        "\n",
        "# Print the predicted label\n",
        "print(f\"The predicted label for the unseen sentence is: {'Positive' if predicted_label == 1 else 'Negative'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zPVqJprSOr8"
      },
      "source": [
        "To perform sentiment analysis using Python's NLTK library, you can utilize the VADER (Valence Aware Dictionary and sEntiment Reasoner) tool, which is included in NLTK. VADER is particularly good for sentiments expressed in social media due to its understanding of slang and emoticons.\n",
        "\n",
        "The SentimentIntensityAnalyzer in NLTK's VADER tool provides four scores: 'negative', 'neutral', 'positive', and 'compound'. The 'compound' score is a single measure that represents the overall sentiment of the sentence, combining the other three scores.\n",
        "\n",
        "The 'compound' score ranges from -1 (most extreme negative) to +1 (most extreme positive). The general rule for interpreting the 'compound' score is as follows:\n",
        "\n",
        "    Positive sentiment: compound score >= 0.05\n",
        "    Neutral sentiment: compound score > -0.05 and < 0.05\n",
        "    Negative sentiment: compound score <= -0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivRRk_6EQ7OL",
        "outputId": "15795218-3ad2-443f-d575-18df28f72041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment scores: {'neg': 0.437, 'neu': 0.563, 'pos': 0.0, 'compound': -0.4779}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Create a SentimentIntensityAnalyzer object\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Sample text for sentiment analysis\n",
        "text = \"i tripped and almost hurt myself\"\n",
        "\n",
        "# Get the sentiment scores\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "\n",
        "print(\"Sentiment scores:\", sentiment_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdUDFRQsTIU7",
        "outputId": "97b6082e-a3c5-4ed4-b4cf-eb7b46311a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: gensim 4.3.2\n",
            "Uninstalling gensim-4.3.2:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.10/dist-packages/gensim-4.3.2.dist-info/*\n",
            "    /usr/local/lib/python3.10/dist-packages/gensim/*\n",
            "Proceed (Y/n)? "
          ]
        }
      ],
      "source": [
        "!pip3 uninstall \"gensim==3.8.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFIPb8_lUqDV",
        "outputId": "5847edff-413b-4c08-ccd8-da5519bb914f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-23.12.11-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.2.2)\n",
            "Building wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=c57bf57e5657c4c503b112c72cad1995bbc83399c2cdb19a9b732c8c1887d05e\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=7547265f45b1909e412b81307bff7b02e0e443741787daa00bca6100755b8154\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-23.12.11 sumy-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sumy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGM1ovtqS5Wz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fccfa5fd-80e3-4a00-c907-87fc0472c142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\".\n",
            "A quip in Tesler's Theorem says \"AI is whatever hasn't been done yet.\"\n",
            "For instance, optical character recognition is frequently excluded from \"artificial intelligence\", having become a routine technology.\n",
            "Capabilities generally classified as AI include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomously operating cars, intelligent routing in content delivery networks, and military simulations.\n",
            "Length of the original text:  1207\n",
            "Length of the summerized text:  697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "# Sample text\n",
        "text = \"\"\"\n",
        "    Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\". As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. A quip in Tesler's Theorem says \"AI is whatever hasn't been done yet.\" For instance, optical character recognition is frequently excluded from \"artificial intelligence\", having become a routine technology. Capabilities generally classified as AI include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomously operating cars, intelligent routing in content delivery networks, and military simulations.\n",
        "    \"\"\"\n",
        "\n",
        "# Sumy requires a Parser object for the text\n",
        "parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
        "\n",
        "# Use the LSA (Latent Semantic Analysis) summarizer\n",
        "summarizer = Summarizer()\n",
        "\n",
        "# Generate summary\n",
        "summary = summarizer(parser.document, 4) # Summarize the text to 3 sentences\n",
        "\n",
        "summary_text = ' '.join([str(sentence) for sentence in summary])\n",
        "\n",
        "# Print the summary\n",
        "print(\"Summary:\")\n",
        "for sentence in summary:\n",
        "    print(sentence)\n",
        "\n",
        "print(\"Length of the original text: \",len(text))\n",
        "\n",
        "print(\"Length of the summerized text: \",len(summary_text))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}