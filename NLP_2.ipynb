{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name Entity Recognition: identifying and categorizing entities in a text corpus into predefined categories such as people, organiations, locations etc."
      ],
      "metadata": {
        "id": "QV3K3DXQbLWZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBxFCWDcHib1",
        "outputId": "36c8a858-8c5a-4e41-e0a6-778c853f89b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "U.K. GPE\n",
            "$1 billion MONEY\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\") # you may use different models: en_core_web_md, en_core_web_lg, en_core_web_trf etc.\n",
        "\n",
        "# Example text\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Iterate over the predicted entities\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming:\n",
        "Chopping the end of the words (Affixes): drives -> drive\n",
        "\n",
        "\n",
        "Lemmatization:\n",
        "This process reduces words to their base or dictionary form, known as the lemma. Running -> Run"
      ],
      "metadata": {
        "id": "EPjXQLQzcfy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize Stemmer and Lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Example word\n",
        "word = [\"running\",\"drives\", \"does\", \"stood\", \"did\", \"studied\"]\n",
        "\n",
        "# Stemming\n",
        "for items in word:\n",
        "  stemmed = stemmer.stem(items)\n",
        "  print(\"Stemmed Word:\", stemmed)\n",
        "\n",
        "# Lemmatization\n",
        "for items in word:\n",
        "  lemmatized = lemmatizer.lemmatize(items, pos='v')\n",
        "  print(\"Lemmatized Word:\", lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJnvpv1gdp8P",
        "outputId": "a243d980-f652-49ea-c337-5585be970441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Word: run\n",
            "Stemmed Word: drive\n",
            "Stemmed Word: doe\n",
            "Stemmed Word: stood\n",
            "Stemmed Word: did\n",
            "Stemmed Word: studi\n",
            "Lemmatized Word: run\n",
            "Lemmatized Word: drive\n",
            "Lemmatized Word: do\n",
            "Lemmatized Word: stand\n",
            "Lemmatized Word: do\n",
            "Lemmatized Word: study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Badge of Words (Feature Extraction)\n",
        "\n",
        " It represents text data by identifying the occurrence of words within the text, disregarding the order or structure of the words.\n",
        "\n",
        " It converts text data into numerical form (vectors), which is necessary for most machine learning algorithms"
      ],
      "metadata": {
        "id": "MbLVi0VCntSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text\n",
        "text = [\"I love writing code in Python\", \"Python is a versatile programming language\"]\n",
        "\n",
        "# Create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "\n",
        "# Summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "# Encode the document\n",
        "vector = vectorizer.transform(text)\n",
        "\n",
        "# Summarize encoded vector\n",
        "print(vector.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsD9ziFnn_UE",
        "outputId": "d992b07a-7685-47a2-fe02-3ec47aee50f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 4, 'writing': 8, 'code': 0, 'in': 1, 'python': 6, 'is': 2, 'versatile': 7, 'programming': 5, 'language': 3}\n",
            "[[1 1 0 0 1 0 1 0 1]\n",
            " [0 0 1 1 0 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (cares about the semantic and the importanc of the words by considering the word in the context)\n"
      ],
      "metadata": {
        "id": "N4xCHmgU2E8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    \"The sky is blue.\",\n",
        "    \"The sun is bright.\"\n",
        "]\n",
        "\n",
        "# Initialize a TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Retrieve the words found in the corpus\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix to an array and display it\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "# Displaying the results\n",
        "tfidf_results = {words[i]: tfidf_array[:, i] for i in range(len(words))}\n",
        "\n",
        "print(tfidf_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhHSG-752Qgb",
        "outputId": "e4136925-8d28-4d31-b628-7e1a00b6829e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'blue': array([0.57615236, 0.        ]), 'bright': array([0.        , 0.57615236]), 'is': array([0.40993715, 0.40993715]), 'sky': array([0.57615236, 0.        ]), 'sun': array([0.        , 0.57615236]), 'the': array([0.40993715, 0.40993715])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrod Embedding:Using NN to provide more informative representationof text"
      ],
      "metadata": {
        "id": "zXDYIeAoBPjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample sentences (replace with your own corpus)\n",
        "sentences = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"I love natural language processing\",\n",
        "    \"Machine learning is fun and interesting\"\n",
        "]\n",
        "\n",
        "# Tokenizing the sentences into words\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "# Train the Word2Vec model with 100 dimensions\n",
        "model = Word2Vec(tokenized_sentences, min_count=1, vector_size=100)\n",
        "\n",
        "# Find words similar to a given word, e.g., 'love'\n",
        "similar_words = model.wv.most_similar('love', topn=5)\n",
        "\n",
        "print(similar_words)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJsqI6Y87-IJ",
        "outputId": "9f1a0f4c-35b6-4af0-a4ed-d2792e36f1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('i', 0.15923377871513367), ('quick', 0.1528114527463913), ('processing', 0.14256368577480316), ('natural', 0.1326463222503662), ('fun', 0.11935518682003021)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sample corpus and labels (replace with your actual data)\n",
        "corpus = [\n",
        "    \"The latest smartphone features a state-of-the-art camera.\",\n",
        "    \"Innovations in AI are transforming the tech industry.\",\n",
        "    \"The new tablet is lightweight and has a long battery life.\",\n",
        "    \"Tech companies are investing heavily in quantum computing.\",\n",
        "    \"Autonomous vehicles could revolutionize daily commutes.\",\n",
        "    \"The election campaign is heating up with debates on policy.\",\n",
        "    \"Legislation for environmental protection was recently passed.\",\n",
        "    \"The new tax law has sparked controversy among businesses.\",\n",
        "    \"Diplomatic talks will address the rising trade tensions.\",\n",
        "    \"The government pledged to increase funding for education.\",\n",
        "    \"The football team clinched victory with a last-minute goal.\",\n",
        "    \"A record-breaking performance in the 100 meters sprint.\",\n",
        "    \"The basketball playoffs are attracting large audiences.\",\n",
        "    \"A new swimming champion has emerged at the international meet.\",\n",
        "    \"The baseball game was postponed due to bad weather.\"\n",
        "]\n",
        "\n",
        "# 0 for Tech, 1 for Politics and 2 for Sport\n",
        "\n",
        "labels = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2)\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000)  # Adjust max_features as needed\n",
        "\n",
        "# Transform training and testing data into TF-IDF vectors\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the SVM classifier\n",
        "clf = SVC(kernel='linear')  # Choose kernel (linear here for simplicity)\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on unseen data (replace with your new sentence)\n",
        "new_sentence = \"Salah did nt score last night\"\n",
        "new_sentence_tfidf = vectorizer.transform([new_sentence])\n",
        "\n",
        "# Get prediction for unseen data\n",
        "prediction = clf.predict(new_sentence_tfidf)\n",
        "\n",
        "# Print the predicted label\n",
        "print(f\"Predicted label for '{new_sentence}': {prediction[0]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ0i1mh8Gj5g",
        "outputId": "cf7301be-b6d3-4b20-d1a7-cfe1f8587e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label for 'Salah did nt score last night': 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Dataset with 20 texts and corresponding labels (1 for positive, 0 for negative)\n",
        "texts = [\n",
        "    \"I love this phone, its super fast and there's so much new stuff to learn!\",\n",
        "    \"The camera quality is amazing, I've taken some great shots.\",\n",
        "    \"I hate this phone, it's always crashing.\",\n",
        "    \"Battery life is terrible, it barely lasts a day.\",\n",
        "    \"Great phone, but I miss having a headphone jack.\",\n",
        "    \"The new update is awesome, my phone works much better now.\",\n",
        "    \"The screen broke after a minor fall, very fragile.\",\n",
        "    \"The voice assistant is surprisingly accurate and helpful!\",\n",
        "    \"I don't like the new interface, it's too complicated.\",\n",
        "    \"Fingerprint sensor is not responsive, I prefer the old scanner.\",\n",
        "    \"This laptop has excellent performance for gaming.\",\n",
        "    \"The sound quality on these headphones is not great.\",\n",
        "    \"I love how easy it is to take photos with this camera.\",\n",
        "    \"The battery on this laptop doesn't last long enough.\",\n",
        "    \"This gaming console is the best I've used so far.\",\n",
        "    \"The touchscreen is unresponsive at times.\",\n",
        "    \"This new app has really helped me organize my schedule.\",\n",
        "    \"The wifi connectivity on this device is poor.\",\n",
        "    \"I'm really impressed with the build quality of this tablet.\",\n",
        "    \"The picture quality on this TV is breathtaking.\"\n",
        "]\n",
        "labels = [1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a pipeline that combines TF-IDF with an SVM classifier\n",
        "pipeline = make_pipeline(TfidfVectorizer(), SVC(kernel='linear'))\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the topic of an unseen sentence\n",
        "unseen_sentence = \"This smartwatch has excellent battery life\"\n",
        "predicted_label = pipeline.predict([unseen_sentence])[0]\n",
        "\n",
        "# Print the predicted label\n",
        "print(f\"The predicted label for the unseen sentence is: {'Positive' if predicted_label == 1 else 'Negative'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5A9S9obGF-n",
        "outputId": "ef446fcf-c43a-4bb6-87a5-9fd672585198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted label for the unseen sentence is: Positive\n"
          ]
        }
      ]
    }
  ]
}